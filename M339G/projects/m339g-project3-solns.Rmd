---
title: "Project #3"
author: "Milica Cudina"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
---
<!-- The author of this template is Dr. Gordan Zitkovic.-->
<!-- The code chunk below contains some settings that will  -->
<!-- make your R code look better in the output pdf file.  -->
<!-- If you are curious about what each option below does, -->
<!-- go to https://yihui.org/knitr/options/ -->
<!-- If not, feel free to disregard everything ...  -->
```{r echo=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align="center",
  fig.pos="t",
  strip.white = TRUE
)
```
<!-- ... down to here. -->

---

## Problem #1 **(5+10+5+10+10+10+10+10+5+25=70 points)**
Solve **Problem 4.8.13** (pp. 192-193) from the textbook. 

*Hint:*  Here is a list of libraries you will need: 

```{r}
library(MASS)
library(ISLR2)
library(e1071)
library(class)
```

*Solution:*
First, here is some exploratory data analysis.

```{r}
summary(Weekly)
cor(Weekly[,-9])
plot(Weekly[, -9], pch=19, col="lightblue")
```

As time goes by, there is more and more trading. So, there is a nice correlation between `Year` and `Volume`. Other than that, I cannot discern a pattern. 

```{r}
mlr.fit <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data = Weekly,
  family = binomial
)
summary(mlr.fit)
```

`Lag2` is the only significant one.

Now, it's time for the **confusion matrix**.

```{r}
probs <- predict(mlr.fit, type = "response") 
glm.pred=rep("Down", length(probs))
glm.pred[probs>0.5]<-"Up"
tab <- table(glm.pred, Weekly$Direction)
tab
sum(diag(tab))/sum(tab)

mean(Weekly$Direction=="Up")
```

The prediction is correct a bit over 56\% of the time. However, the proportion of the realized "Up"s was just under 56\%. So, constantly saying "Up" would work almost as well as our logistic regression. 

Now, for training and testing. 

```{r}
attach(Weekly)
train <- (Year< 2009)
test=Weekly[!train,]
Direction.test=Direction[!train]
dim(test)

fit.tr <- glm(Direction ~ Lag2, data = Weekly, subset=train, family = binomial)

probs.tr <- predict(fit.tr, test[,-9], type = "response")
probs.tr
length(probs.tr)
glm.pred=rep("Down", length(probs.tr))
glm.pred[probs.tr>0.5]<-"Up"
length(glm.pred)

tab <- table(glm.pred, Direction.test)
tab
sum(diag(tab))/sum(tab)
```

Using LDA, we get

```{r}
lda.tr <- lda(Direction ~ Lag2, data = Weekly, subset=train)

lda.tr

lda.pred <- predict(lda.tr, test[,-9], type = "response")$class
#length(lda.pred)
tab <- table(lda.pred, Direction.test)
tab
sum(diag(tab))/sum(tab)
```

For QDA implementation, we just change the command above. 

```{r}
qda.tr <- qda(Direction ~ Lag2, data = Weekly, subset=train)

qda.tr

qda.pred <- predict(qda.tr, test[,-9], type = "response")$class
#length(lda.pred)
tab <- table(qda.pred, Direction.test)
tab
sum(diag(tab))/sum(tab)
```
For KNN, we need to remember that the syntax is slightly different in that the training and the testing are immediately input into the command. 

```{r}
knn.fit <- knn(Weekly[train, "Lag2", drop = FALSE], 
               Weekly[!train, "Lag2", drop = FALSE], 
               Weekly$Direction[train])

tab=table(knn.fit, Direction.test)
tab

sum(diag(tab))/sum(tab)
```

The logistic regression and the LDA - despite being the simplest - perform the best. 

For the remainder of the project, solutions will vary. 
